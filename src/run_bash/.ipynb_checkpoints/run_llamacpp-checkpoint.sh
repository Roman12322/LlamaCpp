python3 -m llama_cpp.server --model ~/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q4_K_M.gguf --n_gpu_layers 50

# # Unsloth
# python3 -m llama_cpp.server --model ~/.cache/huggingface/hub/models--RichardErkhov--unsloth_-_Hermes-2-Pro-Mistral-7B-gguf/snapshots/6e13254bcc521c1cd7b7e72f63f10ac17eaa9d2c/Hermes-2-Pro-Mistral-7B.Q4_K_S.gguf --n_gpu_layers 50